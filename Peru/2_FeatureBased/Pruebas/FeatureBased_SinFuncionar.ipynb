{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatureBased utilizando librerÃ­a TSFEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tsfeature as tsf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = os.path.abspath('')\n",
    "my_path = my_path.split('\\\\')\n",
    "my_path_py = \"\\\\\".join(my_path[:-1])\n",
    "\n",
    "df = pd.read_csv(my_path_py+'\\\\DatosRaw\\\\DEN_2015to2020.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de ciudades con datos es: 505\n"
     ]
    }
   ],
   "source": [
    "print(\"La cantidad de ciudades con datos es:\", len(np.unique(df['Dep-Prov-Distrito'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoding(x):\n",
    "    \"\"\"\n",
    "    :param x: np.array\n",
    "    :return: np.array\n",
    "    \"\"\"\n",
    "    pos, = np.where(np.diff(x) != 0)\n",
    "    pos = np.concatenate(([0], pos+1, [len(x)]))\n",
    "    # rle = [(a,b,x[a]) for (a,b) in zip(pos[:-1],pos[1:])]\n",
    "    rle = [b - a for (a, b) in zip(pos[:-1], pos[1:])]\n",
    "    return rle\n",
    "\n",
    "\n",
    "def hysteresis(x, th_lo, th_hi, initial=False):\n",
    "    \"\"\"\n",
    "    :param x: np.array\n",
    "    :param th_lo: float\n",
    "    :param th_hi: float\n",
    "    :param initial: ???\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    hi = x >= th_hi\n",
    "    lo_or_hi = (x <= th_lo) | hi\n",
    "    ind = np.nonzero(lo_or_hi)[0]\n",
    "    # prevent index error if ind is empty\n",
    "    if not ind.size:\n",
    "        return np.zeros_like(x, dtype=bool) | initial\n",
    "    # from 0 to len(x)\n",
    "    cnt = np.cumsum(lo_or_hi)\n",
    "    return np.where(cnt, hi[ind[cnt-1]], initial)\n",
    "\n",
    "\n",
    "def arg_longest_not_null(x):\n",
    "    # pad with np.nan while finding where null\n",
    "    m = np.concatenate(( [True], np.isnan(x), [True] ))\n",
    "    # Start-stop limits\n",
    "    ss = np.flatnonzero(m[1:] != m[:-1]).reshape(-1,2)\n",
    "    # Get max interval, interval limits\n",
    "    start, stop = ss[(ss[:,1] - ss[:,0]).argmax()]\n",
    "    return start, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class poly(object):\n",
    "    \"\"\" Orthogonal polynomials\n",
    "\n",
    "    Source:\n",
    "        http://davmre.github.io/python/2013/12/15/orthogonal_poly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.degree = None\n",
    "        self.z = None\n",
    "        self.norm2 = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def fit(self, x, degree=1):\n",
    "\n",
    "        self.degree = degree\n",
    "\n",
    "        n = degree + 1\n",
    "        x = np.asarray(x).flatten()\n",
    "        if degree >= len(np.unique(x)):\n",
    "            raise ValueError(\"'degree' must be less than number of unique points\")\n",
    "\n",
    "        xbar = np.mean(x)\n",
    "        x = x - xbar\n",
    "        q, r = np.linalg.qr(np.fliplr(np.vander(x, n)))\n",
    "\n",
    "        z = np.diag(np.diag(r))\n",
    "        raw = np.dot(q, z)\n",
    "\n",
    "        norm2 = np.sum(raw**2, axis=0)\n",
    "        alpha = (np.sum((raw**2)*np.reshape(x, (-1, 1)), axis=0)/norm2 + xbar)[:degree]\n",
    "        z = raw / np.sqrt(norm2)\n",
    "\n",
    "        self.z = z\n",
    "        self.norm2 = norm2\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.asarray(x).flatten()\n",
    "        n = self.degree + 1\n",
    "        z = np.empty((len(x), n))\n",
    "        z[:, 0] = 1\n",
    "\n",
    "        if self.degree > 0:\n",
    "            z[:, 1] = x - self.alpha[0]\n",
    "\n",
    "        if self.degree > 1:\n",
    "            for i in np.arange(1, self.degree):\n",
    "                z[:, i+1] = (x - self.alpha[i]) * z[:, i] - (self.norm2[i] / self.norm2[i-1]) * z[:, i-1]\n",
    "\n",
    "        z /= np.sqrt(self.norm2)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'sliding_window_view' from 'numpy.lib.stride_tricks' (C:\\Users\\gioma\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\lib\\stride_tricks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b6f8eae584af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride_tricks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msliding_window_view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbiplot_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrobust\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'sliding_window_view' from 'numpy.lib.stride_tricks' (C:\\Users\\gioma\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\lib\\stride_tricks.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale as scale_data\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def biplot_features(x, robust=False, scale=True,  col=None, **kwargs):\n",
    "    X = x.dropna(axis=1, how='all').dropna(axis=0, how='any')\n",
    "\n",
    "    if col is None:\n",
    "        col = (\"#000000\", \"darkred\")\n",
    "    else:\n",
    "        col = [col] if not isinstance(col, (list, tuple)) else col\n",
    "        col = np.unique(col)\n",
    "\n",
    "        if len(col) == 1:\n",
    "            col = np.repeat(col, 2)\n",
    "        else:\n",
    "            col = np.unique(col)[0:2]\n",
    "\n",
    "    if scale:\n",
    "        X = scale_data(X, with_mean=True, with_std=True)\n",
    "\n",
    "    if robust:\n",
    "        raise NotImplemented('Robust PCA has not been implemented yet')\n",
    "    else:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(X)\n",
    "        proj_pca = pca.transform(X)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x=proj_pca[:, 0], y=proj_pca[:, 1], c=col)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import boxcox_normmax\n",
    "from statsmodels.sandbox.gam import AdditiveModel\n",
    "\n",
    "try:\n",
    "    from entropy import spectral_entropy\n",
    "except ImportError:\n",
    "    ENTROPY_PACKAGE_AVAILABLE = False\n",
    "else:\n",
    "    ENTROPY_PACKAGE_AVAILABLE = True\n",
    "\n",
    "# features_hyndman\n",
    "# https://github.com/robjhyndman/anomalous/blob/master/R/tsmeasures.R\n",
    "\n",
    "_VARIABLE_COUNT = 0\n",
    "\n",
    "\n",
    "def trim(x, trim=0.1):\n",
    "    \"\"\"Trimmed time series eliminating outliers's influence\"\"\"\n",
    "    qtl = x.quantile([trim, 1 - trim])\n",
    "    lo = qtl.iloc[0]\n",
    "    hi = qtl.iloc[1]\n",
    "\n",
    "    trim_x = x.copy()\n",
    "    trim_x[(trim_x < lo) | (trim_x > hi)] = np.nan\n",
    "    return trim_x\n",
    "\n",
    "\n",
    "def first_order_autocorrelation(x):\n",
    "    \"\"\"First order of autocorrelation\"\"\"\n",
    "    return x.autocorr(1)\n",
    "\n",
    "\n",
    "def lumpiness(x, width):\n",
    "    \"\"\"Lumpiness\n",
    "\n",
    "    Note:\n",
    "        Cannot be used for yearly data\n",
    "    \"\"\"\n",
    "    nr = len(x)\n",
    "    start = np.arange(1, nr, step=width, dtype=int)\n",
    "    end = np.arange(width, nr + width, step=width, dtype=int)\n",
    "\n",
    "    nsegs = int(nr / width)\n",
    "\n",
    "    varx = np.zeros(nsegs)\n",
    "\n",
    "    for idx in range(nsegs):\n",
    "        tmp = x[start[idx]:end[idx]]\n",
    "        varx[idx] = tmp[~np.isnan(tmp)].var()\n",
    "\n",
    "    lump = varx[~np.isnan(varx)].var()\n",
    "    return lump\n",
    "\n",
    "\n",
    "def rolling_level_shift(x, width):\n",
    "    \"\"\"Level shift\n",
    "\n",
    "    Using rolling window\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = x.dropna()\n",
    "    roll_mean = tmp.rolling(width).mean()\n",
    "\n",
    "    try:\n",
    "        level_shifts = roll_mean.diff(width).abs().max()\n",
    "    except Exception:\n",
    "        level_shifts = np.nan\n",
    "\n",
    "    return level_shifts\n",
    "\n",
    "\n",
    "def rolling_variance_change(x, width):\n",
    "    \"\"\"Variance change\n",
    "\n",
    "    Using rolling window\n",
    "\n",
    "    \"\"\"\n",
    "    tmp = x.dropna()\n",
    "\n",
    "    roll_var = tmp.rolling(width).var()\n",
    "\n",
    "    try:\n",
    "        variance_change = roll_var.diff(width).abs().max()\n",
    "    except Exception:\n",
    "        variance_change = np.nan\n",
    "\n",
    "    return variance_change\n",
    "\n",
    "\n",
    "def n_crossing_points(x):\n",
    "    \"\"\"Number of crossing points\"\"\"\n",
    "    mid_line = ((x.max() - x.min()) / 2.0)\n",
    "    ab = (x <= mid_line).values\n",
    "    len_x = len(x)\n",
    "    p1 = ab[1:(len_x - 1)]\n",
    "    p2 = ab[2:len_x]\n",
    "    cross = (p1 & ~p2) | (p2 & ~p1)\n",
    "    return cross.sum()\n",
    "\n",
    "\n",
    "def flat_spots(x):\n",
    "    \"\"\"Flat spots using discretization\"\"\"\n",
    "\n",
    "    try:\n",
    "        cut_x = pd.cut(x, bins=10, include_lowest=True, labels=False)\n",
    "        rle_x = run_length_encoding(cut_x)\n",
    "        spots = max(rle_x)\n",
    "    except Exception:\n",
    "        spots = np.nan\n",
    "\n",
    "    #  Any flat spot\n",
    "    return spots\n",
    "\n",
    "\n",
    "def trend_seasonality_spike_strength(x, freq):\n",
    "    \"\"\"Strength of trend and seasonality and spike\"\"\"\n",
    "    cont_x = x.dropna()\n",
    "    length_cont_x = len(cont_x)\n",
    "    season = peak = trough = np.nan\n",
    "\n",
    "    if length_cont_x < (2 * freq):\n",
    "        trend = linearity = curvature = season = spike = peak = trough = np.nan\n",
    "    else:\n",
    "\n",
    "        if freq > 1:\n",
    "            all_stl = sm.tsa.seasonal_decompose(cont_x, freq=freq)\n",
    "            trend0 = all_stl.trend\n",
    "            fits = trend0 + all_stl.seasonal\n",
    "            adj_x = cont_x - fits\n",
    "            v_adj = adj_x.var()\n",
    "            detrend = cont_x - trend0\n",
    "            deseason = cont_x - all_stl.seasonal\n",
    "            peak = all_stl.seasonal.max()\n",
    "            trough = all_stl.seasonal.min()\n",
    "            remainder = all_stl.resid\n",
    "            season = 0 if detrend.var() < 1e-10 else max(0, min(1, 1 - v_adj/detrend.var()))\n",
    "\n",
    "        else:  # No seasonal component\n",
    "            tt = np.array([range(length_cont_x)]).T\n",
    "\n",
    "            _trend0_values = AdditiveModel(tt).fit(cont_x.values).mu\n",
    "            trend0=pd.Series(_trend0_values, index=cont_x.index)\n",
    "            remainder = cont_x - trend0\n",
    "            deseason = cont_x - trend0\n",
    "            v_adj = trend0.var()\n",
    "\n",
    "        trend = 0 if deseason.var() < 1e-10 else max(0, min(1, 1 - v_adj/deseason.var()))\n",
    "\n",
    "        n = len(remainder)\n",
    "        v = remainder.var()\n",
    "        d = (remainder - remainder.mean())**2\n",
    "        varloo = (v * (n - 1) - d) / (n - 2)\n",
    "        spike = varloo.var()\n",
    "        pl = poly()\n",
    "        pl.fit(range(length_cont_x), degree=2)\n",
    "        result_pl = pl.predict(range(length_cont_x))  # [:, 2]\n",
    "\n",
    "        X = sm.add_constant(result_pl, has_constant='add')\n",
    "        ols_data = trend0.copy()\n",
    "        ols_data = pd.concat([ols_data.reset_index(drop=True), pd.DataFrame(X)], axis=1, ignore_index=True)\n",
    "        ols_data.columns = ['Y', 'Intercept', 'X1', 'X2', 'X3']\n",
    "        result_ols = ols('Y ~ X1 + X2 + X3', data=ols_data.dropna())\n",
    "\n",
    "        trend_coef = result_ols.fit().params\n",
    "        linearity = trend_coef[1]\n",
    "        curvature = trend_coef[2]\n",
    "\n",
    "    result = dict(trend=trend, spike=spike, peak=peak, trough=trough, linearity=linearity, curvature=curvature)\n",
    "\n",
    "    if freq > 1:\n",
    "        result[\"season\"] = season\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def kullback_leibler_score(x, window, threshold=None):\n",
    "    \"\"\"Kullback-Leibler score\"\"\"\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = norm.pdf(38)\n",
    "\n",
    "    gw = 100  # grid width\n",
    "    xgrid = np.arange(x.min(), x.max(), step=(x.max() - x.min()) / gw, dtype=float)\n",
    "    grid = xgrid[1] - xgrid[0]\n",
    "    tmpx = x[~x.isnull()]  # Remove NA to calculate bw\n",
    "    bw = gaussian_kde(tmpx).covariance_factor()\n",
    "    len_x = len(x)\n",
    "\n",
    "    if len_x <= (2 * window):\n",
    "        raise ValueError(\"Cannot compute KLscore when the length is too small.\")\n",
    "\n",
    "    dens_mat = np.zeros((len_x, gw))\n",
    "\n",
    "    for i in range(len_x):\n",
    "        dens_mat[i, :] = norm.pdf(xgrid, x[i], bw)\n",
    "\n",
    "    dens_mat = np.clip(dens_mat, threshold, None)\n",
    "\n",
    "    #ANTESrmean = dens_mat.rolling(window=window).mean()\n",
    "    rmean = sliding_window_view(dens_mat, window_shape = window, axis=1)#.mean()\n",
    "    rmean = rmean.mean(axis=(2))\n",
    "    \n",
    "    lo = range(len_x - window + 1)\n",
    "    hi = range(window + 1, len_x)\n",
    "    seqidx = min(len(lo), len(hi))\n",
    "\n",
    "    kl = np.zeros(seqidx)\n",
    "    for i in range(seqidx):\n",
    "        kl[i] = np.sum(rmean[lo[i], ] * (np.log(rmean[lo[i], ]) - np.log(rmean[hi[i], ])) * grid)\n",
    "\n",
    "    diffkl = pd.Series(kl).dropna().diff()\n",
    "    maxidx = np.argmax(diffkl)\n",
    "\n",
    "    return dict(score=np.max(diffkl), change_idx=maxidx)\n",
    "\n",
    "\n",
    "def boxcox_optimal_lambda(x):\n",
    "    y = x + 0.0000001 if np.any(x == 0) else x\n",
    "    return boxcox_normmax(y)\n",
    "\n",
    "\n",
    "# TODO: implement Spectral Entropy\n",
    "def entropy(x, freq=1, normalize=False):\n",
    "    \"\"\"\n",
    "    Spectral Entropy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start, stop = arg_longest_not_null(x)\n",
    "        result = spectral_entropy(x[start:stop], sf=freq, method='welch', normalize=normalize)\n",
    "    except Exception:\n",
    "        result = np.nan\n",
    "    finally:\n",
    "        return result\n",
    "\n",
    "\n",
    "def ts_measures(x, freq=1, normalize=True, width=None, window=None):\n",
    "    \"\"\"\n",
    "    See `ts_measures_series` doc\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(x, pd.Series):\n",
    "        measures_df = ts_measures_series(x, freq=freq, normalize=normalize, width=width, window=window)\n",
    "    elif isinstance(x, pd.DataFrame):\n",
    "        _buffer = []\n",
    "        for c in x.columns:\n",
    "            _buffer.append(ts_measures_series(x[c], freq=freq, normalize=normalize, width=width, window=window))\n",
    "        measures_df = pd.concat(_buffer, axis=0)\n",
    "\n",
    "    elif issubclass(x.__class__, pd.core.groupby._GroupBy):\n",
    "        _buffer = []\n",
    "        for i in x.groups:\n",
    "            _buffer.append(ts_measures(x.get_group(i), freq=freq, normalize=normalize, width=width, window=window))\n",
    "\n",
    "        measures_df = pd.concat(_buffer, axis=0)\n",
    "    else:\n",
    "        raise TypeError('Unhandled input type')\n",
    "\n",
    "    return measures_df\n",
    "\n",
    "\n",
    "def ts_measures_series(x, freq=1, normalize=True, width=None, window=None):\n",
    "    \"\"\"\n",
    "    :param x: a uni-variate time series\n",
    "    :param freq: number of points to be considered as part of a single period for trend_seasonality_spike_strength\n",
    "    :param normalize: TRUE: scale data to be normally distributed\n",
    "    :param width: a window size for variance change and level shift, lumpiness\n",
    "    :param window: a window size for KLscore\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    name = x.name\n",
    "\n",
    "    if width is None:\n",
    "        width = freq if freq > 1 else 10\n",
    "\n",
    "    if window is None:\n",
    "        window = width\n",
    "\n",
    "    if (width <= 1) | (window <= 1):\n",
    "        raise ValueError(\"Window widths should be greater than 1.\")\n",
    "\n",
    "    # Remove columns containing all NAs\n",
    "    if x.isnull().all():\n",
    "        raise ValueError(\"All values are null\")\n",
    "\n",
    "    if normalize:\n",
    "        x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    trimx = trim(x)\n",
    "\n",
    "    measures = dict()\n",
    "    measures['lumpiness'] = lumpiness(x, width=width)\n",
    "    if ENTROPY_PACKAGE_AVAILABLE:\n",
    "        measures['entropy'] = entropy(x, freq=freq, normalize=False)\n",
    "    measures['ACF1'] = first_order_autocorrelation(x)\n",
    "    measures['lshift'] = rolling_level_shift(trimx, width=width)\n",
    "    measures['vchange'] = rolling_variance_change(trimx, width=width)\n",
    "    measures['cpoints'] = n_crossing_points(x)\n",
    "    measures['fspots'] = flat_spots(x)\n",
    "    #  measures['mean'] = np.mean(x)\n",
    "    #  measures['var'] = np.var(x)\n",
    "\n",
    "    varts = trend_seasonality_spike_strength(x, freq=freq)\n",
    "    measures['trend'] = varts['trend']\n",
    "    measures['linearity'] = varts['linearity']\n",
    "    measures['curvature'] = varts['curvature']\n",
    "    measures['spikiness'] = varts['spike']\n",
    "\n",
    "    if freq > 1:\n",
    "        measures['season'] = varts['season']\n",
    "        measures['peak'] = varts['peak']\n",
    "        measures['trough'] = varts['trough']\n",
    "\n",
    "    threshold = norm.pdf(38)\n",
    "\n",
    "    try:\n",
    "        kl = kullback_leibler_score(x, window=window, threshold=threshold)\n",
    "        measures['KLscore'] = kl['score']\n",
    "        measures['change_idx'] = kl['change_idx']\n",
    "    except Exception:\n",
    "        measures['KLscore'] = np.nan\n",
    "        measures['change_idx'] = np.nan\n",
    "\n",
    "    measures['boxcox'] = boxcox_optimal_lambda(x)\n",
    "\n",
    "    # Build output\n",
    "    measures_df = pd.Series(measures).to_frame().transpose()\n",
    "    measures_df.index = [x.index.min()] if isinstance(x, pd.Series) else [0]\n",
    "    measures_df['variable'] = name if name is not None else generate_name()\n",
    "    return measures_df\n",
    "\n",
    "def generate_name(prefix='var_'):\n",
    "    global _VARIABLE_COUNT\n",
    "    output = \"{}{}\".format(prefix, _VARIABLE_COUNT)\n",
    "    _VARIABLE_COUNT += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "listaDistrito = list(np.unique(df['Dep-Prov-Distrito']))\n",
    "df1 = df[['Dep-Prov-Distrito','Semana', 'Incidencia semanal']]\n",
    "for dis in listaDistrito:\n",
    "    df1.loc[df1['Dep-Prov-Distrito']==dis,'ID_distrito']=i\n",
    "    i=i+1\n",
    "df1= df1.fillna(0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeries = pd.read_csv(my_path_py+'\\\\DatosRaw\\\\SerieTemporal_2015to2020.csv', sep=',')\n",
    "timeSeries = timeSeries.fillna(0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfresh\n",
    "from tsfresh import extract_features\n",
    "\n",
    "#features extraction\n",
    "extracted_features = extract_features(df1, column_id='ID_distrito', column_sort='Semana', column_value='Incidencia semanal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeries = timeSeries.replace('nan', np.nan).fillna(0.0000000001)\n",
    "timeSeries = timeSeries.replace([np.inf, -np.inf], np.nan).fillna(0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfeatures import tsfeatures\n",
    "df2 = df1[['ID_distrito', 'Incidencia semanal', 'Semana']]\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df2.rename(columns = {'ID_distrito':'unique_id', 'Incidencia semanal':'y', 'Semana':'ds'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_s = tsfeatures(df2, freq=4)\n",
    "t_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux2 = timeSeries.loc[[19]].squeeze()\n",
    "aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "\n",
    "Mean=[]\n",
    "Var=[]\n",
    "aCF1=[]\n",
    "Trend=[]\n",
    "Linearity=[]\n",
    "Curvature=[]\n",
    "Season=[]\n",
    "Peak=[]\n",
    "Trough=[]\n",
    "Entropy=[]\n",
    "Lumpiness=[]\n",
    "Spikiness=[]\n",
    "Lshift=[]\n",
    "Vchange=[]\n",
    "Fspots=[]\n",
    "Cpoints_=[]\n",
    "Klscore=[]\n",
    "ChangeIdx=[]\n",
    "\n",
    "for dis in listaDistrito:\n",
    "    casos_distrito1 = df1[df1['Dep-Prov-Distrito']==dis]\n",
    "    id_ = casos_distrito1['ID_distrito'].iloc[1] \n",
    "    casos_distrito1 = casos_distrito1.reset_index(drop=True)\n",
    "    casos_distrito1 = casos_distrito1['Incidencia semanal']\n",
    "    casos_distrito1 = casos_distrito1.replace('nan', np.nan).fillna(0.00000000001)\n",
    "    casos_distrito1 = casos_distrito1.replace(0, np.nan).fillna(0.00000000001)\n",
    "    casos_distrito1 = casos_distrito1.replace([np.inf, -np.inf], np.nan).fillna(0.00000000001)\n",
    "    \n",
    "    ts_aux = t_s[t_s['unique_id'] == id_]\n",
    "    aux2 = timeSeries.loc[[id_]].squeeze()\n",
    "    print(type(aux2))\n",
    "    #Features\n",
    "    mean=tsfresh.feature_extraction.feature_calculators.mean(casos_distrito1)\n",
    "    var=tsfresh.feature_extraction.feature_calculators.variance(casos_distrito1)\n",
    "    ACF1 = ts_aux['e_acf1'].iloc[0]\n",
    "    trend = ts_aux['trend'].iloc[0]\n",
    "    linear = ts_aux['linearity'].iloc[0]\n",
    "    curv = ts_aux['curvature'].iloc[0]\n",
    "    season = ts_aux['seasonal_strength'].iloc[0]\n",
    "    peak = ts_aux['peak'].iloc[0]\n",
    "    trough_ = ts_aux['trough'].iloc[0]\n",
    "    entropy = ts_aux['entropy'].iloc[0]\n",
    "    lump = ts_aux['lumpiness'].iloc[0]\n",
    "    spik = ts_aux['spike'].iloc[0]\n",
    "    #lshift = y[y[\"Distrito\"] == dis].lshift\n",
    "    vchange = rolling_variance_change(casos_distrito1, 4)\n",
    "    fspots = ts_aux['flat_spots'].iloc[0]\n",
    "    cpoints_ = ts_aux['crossing_points'].iloc[0]\n",
    "    klscore = kullback_leibler_score(aux2, 4)['score']\n",
    "    changeidx = kullback_leibler_score(aux2, 4)['change_idx']\n",
    "    \n",
    "    Mean.append(mean)\n",
    "    Var.append(var)\n",
    "    aCF1.append(ACF1)\n",
    "    Trend.append(trend)\n",
    "    Linearity.append(linear)\n",
    "    Curvature.append(curv)\n",
    "    Season.append(season)\n",
    "    Peak.append(peak)\n",
    "    Trough.append(trough_)\n",
    "    Entropy.append(entropy)\n",
    "    Lumpiness.append(lump)\n",
    "    Spikiness.append(spik)\n",
    "    #Lshift.append(lshift[0])\n",
    "    #Vchange.append(vchange[0])\n",
    "    Fspots.append(fspots)\n",
    "    Cpoints_.append(cpoints_)\n",
    "    #Klscore.append(klscore[0])\n",
    "    #ChangeIdx.append(changeidx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(Mean, Var, aCF1, Season, Trend, Linearity, Curvature, Peak, Trough, Entropy, Lumpiness, Spikiness, Fspots, Cpoints_))\n",
    "features = pd.DataFrame(data_tuples, columns =['Mean', 'Var', 'ACF1','Seasonality','Trend', 'Linearity', 'Curvature', 'Peak','Trough', 'Entropy','Lumpiness', 'Spikiness','Fspots', 'Cpoints']) \n",
    "\n",
    "#data_tuples = list(zip(Mean, Var, aCF1, Trend, Linearity, Curvature, Season, Peak, Trough, Entropy, Lumpiness, Spikiness, Lshift, Vchange, Fspots, Cpoints_, Klscore, ChangeIdx))\n",
    "#features = pd.DataFrame(data_tuples, columns =['Mean', 'Var', 'ACF1','Trend', 'Linearity', 'Curvature', 'Season', 'Peak', 'Trough', 'Entropy', 'Lumpiness','Spikiness','Lshift', 'Vchange','Fspots', 'Cpoints', 'KlScore', 'ChangeIdx']) \n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.insert(0, 'Dep-Prov-Distrito', listaDistrito)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.replace('nan', np.nan).fillna(0.00000000001)\n",
    "features = features.replace(0, np.nan).fillna(0.00000000001)\n",
    "features = features.replace([np.inf, -np.inf], np.nan).fillna(0.00000000001)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('18Features_TSF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= timeSeries.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de Distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt, log, floor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "from fastdtw import fastdtw\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#Euclidean\n",
    "def euclidean(x, y):\n",
    "    r=np.linalg.norm(x-y)\n",
    "    if math.isnan(r):\n",
    "        r=1\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "#Fast Dynamic time warping\n",
    "def fast_DTW(x, y):\n",
    "    r, _ = fastdtw(x, y, dist=euclidean)\n",
    "    if math.isnan(r):\n",
    "        r=1\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "#Spearman\n",
    "def scorr(x, y):\n",
    "    r = stats.spearmanr(x, y)[0]\n",
    "    if math.isnan(r):\n",
    "        r=0\n",
    "    #print(r)\n",
    "    return 1 - r\n",
    "\n",
    "#RMSE\n",
    "def rmse(x, y):\n",
    "    r=sqrt(mean_squared_error(x,y))\n",
    "    if math.isnan(r):\n",
    "        r=1\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "def lcs(a, b):  \n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    x, y = len(a), len(b)\n",
    "    result = lengths[x][y]\n",
    "    return result\n",
    "\n",
    "def discretise(x):\n",
    "    return int(x * 10)\n",
    "\n",
    "def multidim_lcs(a, b):\n",
    "    a = a.applymap(discretise)\n",
    "    b = b.applymap(discretise)\n",
    "    rows, dims = a.shape\n",
    "    lcss = [lcs(a[i+2], b[i+2]) for i in range(dims)]\n",
    "    return 1 - sum(lcss) / (rows * dims)\n",
    "\n",
    "#Correlation\n",
    "def corr(x, y):\n",
    "    r=np.dot(x-mean(x),y-mean(y))/((np.linalg.norm(x-mean(x)))*(np.linalg.norm(y-mean(y))))\n",
    "    if math.isnan(r):\n",
    "        r=0\n",
    "    #print(r)\n",
    "    return 1 - r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "DIAMETER_METHODS = ['mean_cluster', 'farthest']\n",
    "CLUSTER_DISTANCE_METHODS = ['nearest', 'farthest']\n",
    "\n",
    "def inter_cluster_distances(labels, distances, method='nearest'):\n",
    "    \"\"\"Calculates the distances between the two nearest points of each cluster.\n",
    "    :param labels: a list containing cluster labels for each of the n elements\n",
    "    :param distances: an n x n numpy.array containing the pairwise distances between elements\n",
    "    :param method: `nearest` for the distances between the two nearest points in each cluster, or `farthest`\n",
    "    \"\"\"\n",
    "    if method not in CLUSTER_DISTANCE_METHODS:\n",
    "        raise ValueError(\n",
    "            'method must be one of {}'.format(CLUSTER_DISTANCE_METHODS))\n",
    "\n",
    "    if method == 'nearest':\n",
    "        return __cluster_distances_by_points(labels, distances)\n",
    "    elif method == 'farthest':\n",
    "        return __cluster_distances_by_points(labels, distances, farthest=True)\n",
    "\n",
    "\n",
    "def __cluster_distances_by_points(labels, distances, farthest=False):\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    cluster_distances = np.full((n_unique_labels, n_unique_labels),\n",
    "                                float('inf') if not farthest else 0)\n",
    "\n",
    "    np.fill_diagonal(cluster_distances, 0)\n",
    "\n",
    "    for i in np.arange(0, len(labels) - 1):\n",
    "        for ii in np.arange(i, len(labels)):\n",
    "            if labels[i] != labels[ii] and (\n",
    "                (not farthest and\n",
    "                 distances[i, ii] < cluster_distances[labels[i], labels[ii]])\n",
    "                    or\n",
    "                (farthest and\n",
    "                 distances[i, ii] > cluster_distances[labels[i], labels[ii]])):\n",
    "                cluster_distances[labels[i], labels[ii]] = cluster_distances[\n",
    "                    labels[ii], labels[i]] = distances[i, ii]\n",
    "    return cluster_distances\n",
    "\n",
    "\n",
    "def diameter(labels, distances, method='farthest'):\n",
    "    \"\"\"Calculates cluster diameters\n",
    "    :param labels: a list containing cluster labels for each of the n elements\n",
    "    :param distances: an n x n numpy.array containing the pairwise distances between elements\n",
    "    :param method: either `mean_cluster` for the mean distance between all elements in each cluster, or `farthest` for the distance between the two points furthest from each other\n",
    "    \"\"\"\n",
    "    if method not in DIAMETER_METHODS:\n",
    "        raise ValueError('method must be one of {}'.format(DIAMETER_METHODS))\n",
    "\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    diameters = np.zeros(n_clusters)\n",
    "\n",
    "    if method == 'mean_cluster':\n",
    "        for i in range(0, len(labels) - 1):\n",
    "            for ii in range(i + 1, len(labels)):\n",
    "                if labels[i] == labels[ii]:\n",
    "                    diameters[labels[i]] += distances[i, ii]\n",
    "\n",
    "        for i in range(len(diameters)):\n",
    "            diameters[i] /= sum(labels == i)\n",
    "\n",
    "    elif method == 'farthest':\n",
    "        for i in range(0, len(labels) - 1):\n",
    "            for ii in range(i + 1, len(labels)):\n",
    "                if labels[i] == labels[ii] and distances[i, ii] > diameters[\n",
    "                        labels[i]]:\n",
    "                    diameters[labels[i]] = distances[i, ii]\n",
    "    return diameters\n",
    "\n",
    "def dunn(labels, distances, diameter_method='farthest',\n",
    "         cdist_method='nearest'):\n",
    "    \"\"\"\n",
    "    Dunn index for cluster validation (larger is better).\n",
    "    \n",
    "    .. math:: D = \\\\min_{i = 1 \\\\ldots n_c; j = i + 1\\ldots n_c} \\\\left\\\\lbrace \\\\frac{d \\\\left( c_i,c_j \\\\right)}{\\\\max_{k = 1 \\\\ldots n_c} \\\\left(diam \\\\left(c_k \\\\right) \\\\right)} \\\\right\\\\rbrace\n",
    "    \n",
    "    where :math:`d(c_i,c_j)` represents the distance between\n",
    "    clusters :math:`c_i` and :math:`c_j`, and :math:`diam(c_k)` is the diameter of cluster :math:`c_k`.\n",
    "    Inter-cluster distance can be defined in many ways, such as the distance between cluster centroids or between their closest elements. Cluster diameter can be defined as the mean distance between all elements in the cluster, between all elements to the cluster centroid, or as the distance between the two furthest elements.\n",
    "    The higher the value of the resulting Dunn index, the better the clustering\n",
    "    result is considered, since higher values indicate that clusters are\n",
    "    compact (small :math:`diam(c_k)`) and far apart (large :math:`d \\\\left( c_i,c_j \\\\right)`).\n",
    "    :param labels: a list containing cluster labels for each of the n elements\n",
    "    :param distances: an n x n numpy.array containing the pairwise distances between elements\n",
    "    :param diameter_method: see :py:function:`diameter` `method` parameter\n",
    "    :param cdist_method: see :py:function:`diameter` `method` parameter\n",
    "    \n",
    "    .. [Kovacs2005] KovÃ¡cs, F., LegÃ¡ny, C., & Babos, A. (2005). Cluster validity measurement techniques. 6th International Symposium of Hungarian Researchers on Computational Intelligence.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = LabelEncoder().fit(labels).transform(labels)\n",
    "    \n",
    "    \n",
    "\n",
    "    ic_distances = inter_cluster_distances(labels, distances, cdist_method)\n",
    "    #print(\"IC\",ic_distances)\n",
    "    if len(ic_distances[ic_distances.nonzero()])==0:\n",
    "        min_distance = 0\n",
    "    else:\n",
    "        min_distance = min(ic_distances[ic_distances.nonzero()])\n",
    "    max_diameter = max(diameter(labels, distances, diameter_method))\n",
    "    \n",
    "    \n",
    "\n",
    "    return min_distance / max_diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.replace('nan', np.nan).fillna(0.00000000001)\n",
    "features = features.drop('Distrito', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DefiniciÃ³n del nÃºmero de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices de distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HAC_EUCLIDEAN=[]\n",
    "HAC_CORRELATION=[]\n",
    "HAC_SPEARMAN=[]\n",
    "HAC_DTW=[]\n",
    "\n",
    "#Euclidean\n",
    "f_euclidean_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    #print(\"i\",i)\n",
    "    for j in range(1,n):\n",
    "         f_euclidean_dist[i,j] = euclidean(features.iloc[i].values.flatten(), features.iloc[j].values.flatten())\n",
    "\n",
    "#Corr\n",
    "corr_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "            corr_dist[i,j] = corr(features.iloc[i].values.flatten(), features.iloc[j].values.flatten())\n",
    "\n",
    "#scorr\n",
    "f_scorr_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        f_scorr_dist[i,j] = scorr(features.iloc[i].values.flatten(), features.iloc[j].values.flatten())\n",
    "#DTW\n",
    "f_dtw_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        f_dtw_dist[i,j] = fast_DTW(features.iloc[i].values.flatten(), features.iloc[j].values.flatten())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAC Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimentos HAC\n",
    "HAC_euc = AgglomerativeClustering(n_clusters=k).fit_predict(f_euclidean_dist)\n",
    "print(\"HAC + euclidian distance: \")\n",
    "print(\"SC: \",silhouette_score(f_euclidean_dist, HAC_euc))\n",
    "HAC_EUCLIDEAN.append(silhouette_score(f_euclidean_dist, HAC_euc))\n",
    "\n",
    "HAC_corr = AgglomerativeClustering(n_clusters=k).fit_predict(corr_dist)\n",
    "print(\"HAC + corr distance: \")\n",
    "print(\"SC: \",silhouette_score(corr_dist,HAC_corr))\n",
    "HAC_CORRELATION.append(silhouette_score(corr_dist, HAC_euc))\n",
    "\n",
    "HAC_scorr = AgglomerativeClustering(n_clusters=k).fit_predict(f_scorr_dist)\n",
    "print(\"HAC + scorr distance: \")\n",
    "print(\"SC: \",silhouette_score(f_scorr_dist, HAC_scorr))\n",
    "HAC_SPEARMAN.append(silhouette_score(f_scorr_dist, HAC_scorr))\n",
    "\n",
    "HAC_dtw = AgglomerativeClustering(n_clusters=k).fit_predict(f_dtw_dist)\n",
    "print(\"HAC + dtw distance: \")\n",
    "print(\"SC: \",silhouette_score(f_dtw_dist, HAC_dtw))\n",
    "HAC_DTW.append(silhouette_score(f_dtw_dist, HAC_dtw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_EUCLIDEAN=[]\n",
    "KM_CORRELATION=[]\n",
    "KM_SPEARMAN=[]\n",
    "KM_DTW=[]\n",
    "    \n",
    "#Experimentos K-Means\n",
    "km_euc = KMeans(n_clusters=k).fit_predict(f_euclidean_dist)\n",
    "#print(km_euc.shape)\n",
    "print(\"KM + euclidian distance: \")\n",
    "print(\"SC: \",silhouette_score(f_euclidean_dist, km_euc))\n",
    "KM_EUCLIDEAN.append(silhouette_score(f_euclidean_dist, km_euc))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "km_corr = KMeans(n_clusters=k).fit_predict(corr_dist)\n",
    "\n",
    "\n",
    "print(\"KM + corr distance: \")\n",
    "print(\"SC: \",silhouette_score(corr_dist, km_corr))\n",
    "KM_CORRELATION.append(silhouette_score(corr_dist, km_corr))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "km_scorr = KMeans(n_clusters=k).fit_predict(f_scorr_dist)\n",
    "print(\"KM + scorr distance: \")\n",
    "print(\"SC: \",silhouette_score(f_scorr_dist, km_scorr))\n",
    "KM_SPEARMAN.append(silhouette_score(f_scorr_dist, km_scorr))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "km_dtw = KMeans(n_clusters=k).fit_predict(f_dtw_dist)\n",
    "print(\"KM + dtw distance: \")\n",
    "print(\"SC: \",silhouette_score(f_dtw_dist, km_dtw))\n",
    "KM_DTW.append(silhouette_score(f_dtw_dist, km_dtw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBScan Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_EUCLIDEAN=[]\n",
    "DBSCAN_CORRELATION=[]\n",
    "DBSCAN_SPEARMAN=[]\n",
    "DBSCAN_DTW=[]\n",
    "\n",
    "#CON EUCLIDEAN\n",
    "DB_euc = DBSCAN(eps=0.5, min_samples=36).fit_predict(f_euclidean_dist)\n",
    "print(\"DBSCAN + euclidian distance: \")\n",
    "print(\"SC: \",silhouette_score(f_euclidean_dist, DB_euc))\n",
    "DBSCAN_EUCLIDEAN.append(silhouette_score(f_euclidean_dist, DB_euc))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "#CON CORRELATION\n",
    "DB_corr = DBSCAN(eps=0.5, min_samples=36).fit_predict(corr_dist)\n",
    "print(\"DBSCAN + corr distance: \")\n",
    "print(\"SC: \",silhouette_score(corr_dist, DB_corr))\n",
    "DBSCAN_CORRELATION.append(silhouette_score(corr_dist, DB_corr))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "#CON SPEARMAN\n",
    "DB_scorr = DBSCAN(eps=0.5, min_samples=36).fit_predict(f_scorr_dist)\n",
    "print(\"DBSCAN + scorr distance: \")\n",
    "print(\"SC: \",silhouette_score(f_scorr_dist, DB_scorr))\n",
    "DBSCAN_SPEARMAN.append(silhouette_score(f_scorr_dist, DB_scorr))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "#CON D TIME WARPING\n",
    "DB_dtw = DBSCAN(eps=0.5, min_samples=36).fit_predict(f_dtw_dist)\n",
    "print(\"DBSCAN + dtw distance: \")\n",
    "print(\"SC: \",silhouette_score(f_dtw_dist, DB_dtw))\n",
    "DBSCAN_DTW.append(silhouette_score(f_dtw_dist, DB_dtw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KM length: \", len(KM_EUCLIDEAN))\n",
    "print(\"HAC length: \", len(HAC_EUCLIDEAN))\n",
    "print(\"DBSCAN length: \", len(DBSCAN_EUCLIDEAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = pd.DataFrame()\n",
    "excluded_metric = ['None','Mean', 'Var', 'ACF1','Trend', 'Linearity', 'Curvature', 'Peak','Entropy','Lumpiness','Lshift', 'Vchange','Fspots', 'Cpoints_ts', 'Cpoints_JV', 'KlScore', 'ChangeIdx']\n",
    "sil_scores['ExcludedMetric'] = np.array(excluded_metric)\n",
    "sil_scores['EUCLIDEAN-HAC'] = np.array(HAC_EUCLIDEAN)\n",
    "sil_scores['CORRELATION-HAC'] = np.array(HAC_CORRELATION)\n",
    "sil_scores['SPEARMAN-HAC'] = np.array(HAC_SPEARMAN)\n",
    "sil_scores['DTW-HAC'] = np.array(HAC_DTW)\n",
    "sil_scores['EUCLIDEAN-KM'] = np.array(KM_EUCLIDEAN)\n",
    "sil_scores['CORRELATION-KM'] = np.array(KM_CORRELATION)\n",
    "sil_scores['SPEARMAN-KM'] = np.array(KM_SPEARMAN)\n",
    "sil_scores['DTW-KM'] = np.array(KM_DTW)\n",
    "sil_scores['EUCLIDEAN-DB'] = np.array(DBSCAN_EUCLIDEAN)\n",
    "sil_scores['CORRELATION-DB'] = np.array(DBSCAN_CORRELATION)\n",
    "sil_scores['SPEARMAN-DB'] = np.array(DBSCAN_SPEARMAN)\n",
    "sil_scores['DTW-DB'] = np.array(DBSCAN_DTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores.to_csv('SilScoreFB_TSF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
