{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECCION DE ATRIBUTOS\n",
    "\n",
    "## METODO DEL FILTRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "my_path = os.path.abspath('')\n",
    "my_path = my_path.split('\\\\')\n",
    "my_path_py = \"\\\\\".join(my_path[:-1])\n",
    "\n",
    "features = pd.read_csv(my_path_py+'\\\\2_FeatureBased\\\\FB2_2009al2013.csv', sep=',')\n",
    "Puntajes_allf = pd.read_csv(my_path_py+'\\\\2_FeatureBased\\\\Scores1_FeatureBasedK6.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distrito</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "      <th>ACF1</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Linearity</th>\n",
       "      <th>Curvature</th>\n",
       "      <th>Season</th>\n",
       "      <th>Peak</th>\n",
       "      <th>Trough</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Lumpiness</th>\n",
       "      <th>Spikiness</th>\n",
       "      <th>Lshift</th>\n",
       "      <th>Vchange</th>\n",
       "      <th>Fspots</th>\n",
       "      <th>Cpoints</th>\n",
       "      <th>KlScore</th>\n",
       "      <th>ChangeIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUNCION</td>\n",
       "      <td>6.904494</td>\n",
       "      <td>5.239362e+01</td>\n",
       "      <td>8.135193e-01</td>\n",
       "      <td>0.922999</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>-0.863262</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>5.978370e-01</td>\n",
       "      <td>1.772715e-04</td>\n",
       "      <td>2.497263e-09</td>\n",
       "      <td>1.086957e-02</td>\n",
       "      <td>5.250998e-05</td>\n",
       "      <td>172.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FERNANDO DE LA MORA</td>\n",
       "      <td>44.781615</td>\n",
       "      <td>4.841265e+03</td>\n",
       "      <td>9.371226e-01</td>\n",
       "      <td>0.958335</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.391511</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.010903</td>\n",
       "      <td>1.698908e-01</td>\n",
       "      <td>7.906015e-05</td>\n",
       "      <td>5.201407e-10</td>\n",
       "      <td>1.813156e-01</td>\n",
       "      <td>1.006363e-02</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PEDRO JUAN CABALLERO</td>\n",
       "      <td>44.819130</td>\n",
       "      <td>1.042302e+04</td>\n",
       "      <td>9.285754e-01</td>\n",
       "      <td>0.962853</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>-0.361289</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.004989</td>\n",
       "      <td>1.317796e-01</td>\n",
       "      <td>3.606045e-04</td>\n",
       "      <td>5.397936e-10</td>\n",
       "      <td>6.596210e-02</td>\n",
       "      <td>3.044544e-03</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YBY YAU</td>\n",
       "      <td>18.306891</td>\n",
       "      <td>8.273701e+02</td>\n",
       "      <td>7.051710e-01</td>\n",
       "      <td>0.879186</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>-0.677443</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.018949</td>\n",
       "      <td>5.381565e-01</td>\n",
       "      <td>3.756719e-04</td>\n",
       "      <td>3.576345e-09</td>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>2.083333e-03</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YPANE</td>\n",
       "      <td>59.676847</td>\n",
       "      <td>1.191727e+04</td>\n",
       "      <td>9.103049e-01</td>\n",
       "      <td>0.940326</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>-0.154082</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.005024</td>\n",
       "      <td>1.524750e-01</td>\n",
       "      <td>8.255672e-05</td>\n",
       "      <td>1.405710e-09</td>\n",
       "      <td>1.234592e-01</td>\n",
       "      <td>7.082626e-03</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>ISLA UMBU</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3.203435e-11</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>VILLA OLIVA</td>\n",
       "      <td>27.129680</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3.203435e-11</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>CARMELO PERALTA</td>\n",
       "      <td>23.540490</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.899220</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>-0.144232</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>2.266590e-03</td>\n",
       "      <td>5.186532e-09</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>191.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>DESMOCHADOS</td>\n",
       "      <td>56.497175</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3.203435e-11</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>R I 3 CORRALES</td>\n",
       "      <td>10.999890</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3.203435e-11</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Distrito       Mean           Var          ACF1     Trend  \\\n",
       "0                ASUNCION   6.904494  5.239362e+01  8.135193e-01  0.922999   \n",
       "1     FERNANDO DE LA MORA  44.781615  4.841265e+03  9.371226e-01  0.958335   \n",
       "2    PEDRO JUAN CABALLERO  44.819130  1.042302e+04  9.285754e-01  0.962853   \n",
       "3                 YBY YAU  18.306891  8.273701e+02  7.051710e-01  0.879186   \n",
       "4                   YPANE  59.676847  1.191727e+04  9.103049e-01  0.940326   \n",
       "..                    ...        ...           ...           ...       ...   \n",
       "195             ISLA UMBU  33.333333  1.000000e-07  1.000000e-08  0.984279   \n",
       "196           VILLA OLIVA  27.129680  1.000000e-07  1.000000e-08  0.984279   \n",
       "197       CARMELO PERALTA  23.540490  1.000000e-07  1.000000e-08  0.899220   \n",
       "198           DESMOCHADOS  56.497175  1.000000e-07  1.000000e-08  0.984279   \n",
       "199        R I 3 CORRALES  10.999890  1.000000e-07  1.000000e-08  0.984279   \n",
       "\n",
       "     Linearity  Curvature    Season          Peak    Trough       Entropy  \\\n",
       "0     0.003082  -0.863262  0.025780  1.000000e+00 -0.010495  5.978370e-01   \n",
       "1     0.007501   0.391511  0.046909  1.000000e-07 -0.010903  1.698908e-01   \n",
       "2     0.004186  -0.361289  0.010942  1.000000e-07 -0.004989  1.317796e-01   \n",
       "3     0.003470  -0.677443  0.049205  1.000000e+00 -0.018949  5.381565e-01   \n",
       "4     0.005569  -0.154082  0.019642  1.000000e-07 -0.005024  1.524750e-01   \n",
       "..         ...        ...       ...           ...       ...           ...   \n",
       "195   0.000052  -0.016121  0.015653  1.000000e-07 -0.001953  1.000000e-08   \n",
       "196   0.000052  -0.016121  0.015653  1.000000e-07 -0.001953  1.000000e-08   \n",
       "197   0.000466  -0.144232  0.020585  1.000000e-07 -0.007148  1.000000e-08   \n",
       "198   0.000052  -0.016121  0.015653  1.000000e-07 -0.001953  1.000000e-08   \n",
       "199   0.000052  -0.016121  0.015653  1.000000e-07 -0.001953  1.000000e-08   \n",
       "\n",
       "        Lumpiness     Spikiness        Lshift       Vchange  Fspots  Cpoints  \\\n",
       "0    1.772715e-04  2.497263e-09  1.086957e-02  5.250998e-05   172.0      6.0   \n",
       "1    7.906015e-05  5.201407e-10  1.813156e-01  1.006363e-02    49.0     10.0   \n",
       "2    3.606045e-04  5.397936e-10  6.596210e-02  3.044544e-03    58.0      2.0   \n",
       "3    3.756719e-04  3.576345e-09  5.000000e-02  2.083333e-03   120.0      3.0   \n",
       "4    8.255672e-05  1.405710e-09  1.234592e-01  7.082626e-03    84.0      4.0   \n",
       "..            ...           ...           ...           ...     ...      ...   \n",
       "195  1.000000e-07  3.203435e-11  1.000000e-07  1.000000e-07   193.0      0.0   \n",
       "196  1.000000e-07  3.203435e-11  1.000000e-07  1.000000e-07   193.0      0.0   \n",
       "197  2.266590e-03  5.186532e-09  1.000000e-07  1.000000e-07   191.0      1.0   \n",
       "198  1.000000e-07  3.203435e-11  1.000000e-07  1.000000e-07   193.0      0.0   \n",
       "199  1.000000e-07  3.203435e-11  1.000000e-07  1.000000e-07   193.0      0.0   \n",
       "\n",
       "          KlScore     ChangeIdx  \n",
       "0    1.000000e-10  1.000000e-10  \n",
       "1    1.000000e-10  1.000000e-10  \n",
       "2    1.000000e-10  1.000000e-10  \n",
       "3    1.000000e-10  1.000000e-10  \n",
       "4    1.000000e-10  1.000000e-10  \n",
       "..            ...           ...  \n",
       "195  1.000000e-10  1.000000e-10  \n",
       "196  1.000000e-10  1.000000e-10  \n",
       "197  1.000000e-10  1.000000e-10  \n",
       "198  1.000000e-10  1.000000e-10  \n",
       "199  1.000000e-10  1.000000e-10  \n",
       "\n",
       "[200 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = features.iloc[:, 1:]\n",
    "listadistrito = features.Distrito.values\n",
    "features = features.drop('Distrito', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las dimensiones de nuestros datos son:  (200, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Las dimensiones de nuestros datos son: \", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar features varianza casi nula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "f_selection = sel.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of VarianceThreshold(threshold=0.001)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las dimensiones de nuestros datos seleccionados son:  (200, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Las dimensiones de nuestros datos seleccionados son: \", f_selection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.90449449e+00,  5.23936187e+01,  8.13519297e-01,  9.22998773e-01,\n",
       "       -8.63262230e-01,  1.00000000e+00,  5.97837001e-01,  1.08695652e-02,\n",
       "        1.72000000e+02,  6.00000000e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_selection[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "      <th>ACF1</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Curvature</th>\n",
       "      <th>Peak</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Lshift</th>\n",
       "      <th>Fspots</th>\n",
       "      <th>Cpoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.904494</td>\n",
       "      <td>5.239362e+01</td>\n",
       "      <td>8.135193e-01</td>\n",
       "      <td>0.922999</td>\n",
       "      <td>-0.863262</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.978370e-01</td>\n",
       "      <td>1.086957e-02</td>\n",
       "      <td>172.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.781615</td>\n",
       "      <td>4.841265e+03</td>\n",
       "      <td>9.371226e-01</td>\n",
       "      <td>0.958335</td>\n",
       "      <td>0.391511</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.698908e-01</td>\n",
       "      <td>1.813156e-01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.819130</td>\n",
       "      <td>1.042302e+04</td>\n",
       "      <td>9.285754e-01</td>\n",
       "      <td>0.962853</td>\n",
       "      <td>-0.361289</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.317796e-01</td>\n",
       "      <td>6.596210e-02</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.306891</td>\n",
       "      <td>8.273701e+02</td>\n",
       "      <td>7.051710e-01</td>\n",
       "      <td>0.879186</td>\n",
       "      <td>-0.677443</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.381565e-01</td>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.676847</td>\n",
       "      <td>1.191727e+04</td>\n",
       "      <td>9.103049e-01</td>\n",
       "      <td>0.940326</td>\n",
       "      <td>-0.154082</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.524750e-01</td>\n",
       "      <td>1.234592e-01</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>33.333333</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>27.129680</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>23.540490</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.899220</td>\n",
       "      <td>-0.144232</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>191.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>56.497175</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>10.999890</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.984279</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Mean           Var          ACF1     Trend  Curvature          Peak  \\\n",
       "0     6.904494  5.239362e+01  8.135193e-01  0.922999  -0.863262  1.000000e+00   \n",
       "1    44.781615  4.841265e+03  9.371226e-01  0.958335   0.391511  1.000000e-07   \n",
       "2    44.819130  1.042302e+04  9.285754e-01  0.962853  -0.361289  1.000000e-07   \n",
       "3    18.306891  8.273701e+02  7.051710e-01  0.879186  -0.677443  1.000000e+00   \n",
       "4    59.676847  1.191727e+04  9.103049e-01  0.940326  -0.154082  1.000000e-07   \n",
       "..         ...           ...           ...       ...        ...           ...   \n",
       "195  33.333333  1.000000e-07  1.000000e-08  0.984279  -0.016121  1.000000e-07   \n",
       "196  27.129680  1.000000e-07  1.000000e-08  0.984279  -0.016121  1.000000e-07   \n",
       "197  23.540490  1.000000e-07  1.000000e-08  0.899220  -0.144232  1.000000e-07   \n",
       "198  56.497175  1.000000e-07  1.000000e-08  0.984279  -0.016121  1.000000e-07   \n",
       "199  10.999890  1.000000e-07  1.000000e-08  0.984279  -0.016121  1.000000e-07   \n",
       "\n",
       "          Entropy        Lshift  Fspots  Cpoints  \n",
       "0    5.978370e-01  1.086957e-02   172.0      6.0  \n",
       "1    1.698908e-01  1.813156e-01    49.0     10.0  \n",
       "2    1.317796e-01  6.596210e-02    58.0      2.0  \n",
       "3    5.381565e-01  5.000000e-02   120.0      3.0  \n",
       "4    1.524750e-01  1.234592e-01    84.0      4.0  \n",
       "..            ...           ...     ...      ...  \n",
       "195  1.000000e-08  1.000000e-07   193.0      0.0  \n",
       "196  1.000000e-08  1.000000e-07   193.0      0.0  \n",
       "197  1.000000e-08  1.000000e-07   191.0      1.0  \n",
       "198  1.000000e-08  1.000000e-07   193.0      0.0  \n",
       "199  1.000000e-08  1.000000e-07   193.0      0.0  \n",
       "\n",
       "[200 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(f_selection, columns = ['Mean','Var','ACF1', 'Trend', 'Curvature', 'Peak', 'Entropy', 'Lshift', 'Fspots','Cpoints'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar features con alta correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por convención valores mayores a 10 se consideran redundantes\n",
      " \n",
      "Mean tiene  1.9113631329809755\n",
      "Var tiene  1.9929487295759996\n",
      "ACF1 tiene  3.0577032511119375\n",
      "Trend tiene  6.46180204763918\n",
      "Linearity tiene  18.359553094595533\n",
      "Curvature tiene  3.9196849287558124\n",
      "Season tiene  2.528956944026519\n",
      "Peak tiene  1.4969862917433463\n",
      "Trough tiene  4.317277203118787\n",
      "Entropy tiene  1.7149790252917756\n",
      "Lumpiness tiene  1.2652314451585345\n",
      "Spikiness tiene  7.418825466771216\n",
      "Lshift tiene  12.774641238979077\n",
      "Vchange tiene  10.350707711154586\n",
      "Fspots tiene  10.620086105118045\n",
      "Cpoints tiene  4.920434693497325\n",
      "KlScore tiene  0.0\n",
      "ChangeIdx tiene  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gioma\\AppData\\Roaming\\Python\\Python38\\site-packages\\statsmodels\\regression\\linear_model.py:1736: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_scores = [variance_inflation_factor(features.values, feature) for feature in range(len(features.columns))]\n",
    "print(\"Por convención valores mayores a 10 se consideran redundantes\")\n",
    "print(\" \")\n",
    "i=0\n",
    "for c in features.columns:\n",
    "    print(c, \"tiene \", vif_scores[i])\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por convención valores mayores a 10 se consideran redundantes\n",
      " \n",
      "Mean tiene  4.452790748229568\n",
      "Var tiene  2.0777774673728233\n",
      "ACF1 tiene  3.0164507826053235\n",
      "Trend tiene  51.82870232416742\n",
      "Curvature tiene  5.938035982570322\n",
      "Peak tiene  1.5611216287343512\n",
      "Entropy tiene  3.4219853514485563\n",
      "Lshift tiene  3.537602459866726\n",
      "Fspots tiene  49.33668910425736\n",
      "Cpoints tiene  6.265423092403623\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_scores = [variance_inflation_factor(df.values, feature) for feature in range(len(df.columns))]\n",
    "print(\"Por convención valores mayores a 10 se consideran redundantes\")\n",
    "print(\" \")\n",
    "i=0\n",
    "for c in df.columns:\n",
    "    print(c, \"tiene \", vif_scores[i])\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FeaturesUnsupervised.csv', 'w') as f:\n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt, log, floor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "from fastdtw import fastdtw\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    " \n",
    "#Euclidean\n",
    "def euclidean(x, y):\n",
    "    r=np.linalg.norm(x-y)\n",
    "    if math.isnan(r):\n",
    "        r=1\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "#Fast Dynamic time warping\n",
    "def fast_DTW(x, y):\n",
    "    r, _ = fastdtw(x, y, dist=euclidean)\n",
    "    if math.isnan(r):\n",
    "        r=1\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "#Spearman\n",
    "def scorr(x, y):\n",
    "    r = stats.spearmanr(x, y)[0]\n",
    "    if math.isnan(r):\n",
    "        r=0\n",
    "    #print(r)\n",
    "    return 1 - r\n",
    "\n",
    "#RMSE\n",
    "def rmse(x, y):\n",
    "    r=sqrt(mean_squared_error(x,y))\n",
    "    if math.isnan(r):\n",
    "        r=1\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "def lcs(a, b):  \n",
    "    lengths = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n",
    "    # row 0 and column 0 are initialized to 0 already\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    x, y = len(a), len(b)\n",
    "    result = lengths[x][y]\n",
    "    return result\n",
    "\n",
    "def discretise(x):\n",
    "    return int(x * 10)\n",
    "\n",
    "def multidim_lcs(a, b):\n",
    "    a = a.applymap(discretise)\n",
    "    b = b.applymap(discretise)\n",
    "    rows, dims = a.shape\n",
    "    lcss = [lcs(a[i+2], b[i+2]) for i in range(dims)]\n",
    "    return 1 - sum(lcss) / (rows * dims)\n",
    "\n",
    "#Correlation\n",
    "def corr(x, y):\n",
    "    r=np.dot(x-mean(x),y-mean(y))/((np.linalg.norm(x-mean(x)))*(np.linalg.norm(y-mean(y))))\n",
    "    if math.isnan(r):\n",
    "        r=0\n",
    "    #print(r)\n",
    "    return 1 - r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "DIAMETER_METHODS = ['mean_cluster', 'farthest']\n",
    "CLUSTER_DISTANCE_METHODS = ['nearest', 'farthest']\n",
    "\n",
    "def inter_cluster_distances(labels, distances, method='nearest'):\n",
    "    \"\"\"Calculates the distances between the two nearest points of each cluster.\n",
    "    :param labels: a list containing cluster labels for each of the n elements\n",
    "    :param distances: an n x n numpy.array containing the pairwise distances between elements\n",
    "    :param method: `nearest` for the distances between the two nearest points in each cluster, or `farthest`\n",
    "    \"\"\"\n",
    "    if method not in CLUSTER_DISTANCE_METHODS:\n",
    "        raise ValueError(\n",
    "            'method must be one of {}'.format(CLUSTER_DISTANCE_METHODS))\n",
    "\n",
    "    if method == 'nearest':\n",
    "        return __cluster_distances_by_points(labels, distances)\n",
    "    elif method == 'farthest':\n",
    "        return __cluster_distances_by_points(labels, distances, farthest=True)\n",
    "\n",
    "\n",
    "def __cluster_distances_by_points(labels, distances, farthest=False):\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    cluster_distances = np.full((n_unique_labels, n_unique_labels),\n",
    "                                float('inf') if not farthest else 0)\n",
    "\n",
    "    np.fill_diagonal(cluster_distances, 0)\n",
    "\n",
    "    for i in np.arange(0, len(labels) - 1):\n",
    "        for ii in np.arange(i, len(labels)):\n",
    "            if labels[i] != labels[ii] and (\n",
    "                (not farthest and\n",
    "                 distances[i, ii] < cluster_distances[labels[i], labels[ii]])\n",
    "                    or\n",
    "                (farthest and\n",
    "                 distances[i, ii] > cluster_distances[labels[i], labels[ii]])):\n",
    "                cluster_distances[labels[i], labels[ii]] = cluster_distances[\n",
    "                    labels[ii], labels[i]] = distances[i, ii]\n",
    "    return cluster_distances\n",
    "\n",
    "\n",
    "def diameter(labels, distances, method='farthest'):\n",
    "    \"\"\"Calculates cluster diameters\n",
    "    :param labels: a list containing cluster labels for each of the n elements\n",
    "    :param distances: an n x n numpy.array containing the pairwise distances between elements\n",
    "    :param method: either `mean_cluster` for the mean distance between all elements in each cluster, or `farthest` for the distance between the two points furthest from each other\n",
    "    \"\"\"\n",
    "    if method not in DIAMETER_METHODS:\n",
    "        raise ValueError('method must be one of {}'.format(DIAMETER_METHODS))\n",
    "\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    diameters = np.zeros(n_clusters)\n",
    "\n",
    "    if method == 'mean_cluster':\n",
    "        for i in range(0, len(labels) - 1):\n",
    "            for ii in range(i + 1, len(labels)):\n",
    "                if labels[i] == labels[ii]:\n",
    "                    diameters[labels[i]] += distances[i, ii]\n",
    "\n",
    "        for i in range(len(diameters)):\n",
    "            diameters[i] /= sum(labels == i)\n",
    "\n",
    "    elif method == 'farthest':\n",
    "        for i in range(0, len(labels) - 1):\n",
    "            for ii in range(i + 1, len(labels)):\n",
    "                if labels[i] == labels[ii] and distances[i, ii] > diameters[\n",
    "                        labels[i]]:\n",
    "                    diameters[labels[i]] = distances[i, ii]\n",
    "    return diameters\n",
    "\n",
    "def dunn(labels, distances, diameter_method='farthest',\n",
    "         cdist_method='nearest'):\n",
    "    \"\"\"\n",
    "    Dunn index for cluster validation (larger is better).\n",
    "    \n",
    "    .. math:: D = \\\\min_{i = 1 \\\\ldots n_c; j = i + 1\\ldots n_c} \\\\left\\\\lbrace \\\\frac{d \\\\left( c_i,c_j \\\\right)}{\\\\max_{k = 1 \\\\ldots n_c} \\\\left(diam \\\\left(c_k \\\\right) \\\\right)} \\\\right\\\\rbrace\n",
    "    \n",
    "    where :math:`d(c_i,c_j)` represents the distance between\n",
    "    clusters :math:`c_i` and :math:`c_j`, and :math:`diam(c_k)` is the diameter of cluster :math:`c_k`.\n",
    "    Inter-cluster distance can be defined in many ways, such as the distance between cluster centroids or between their closest elements. Cluster diameter can be defined as the mean distance between all elements in the cluster, between all elements to the cluster centroid, or as the distance between the two furthest elements.\n",
    "    The higher the value of the resulting Dunn index, the better the clustering\n",
    "    result is considered, since higher values indicate that clusters are\n",
    "    compact (small :math:`diam(c_k)`) and far apart (large :math:`d \\\\left( c_i,c_j \\\\right)`).\n",
    "    :param labels: a list containing cluster labels for each of the n elements\n",
    "    :param distances: an n x n numpy.array containing the pairwise distances between elements\n",
    "    :param diameter_method: see :py:function:`diameter` `method` parameter\n",
    "    :param cdist_method: see :py:function:`diameter` `method` parameter\n",
    "    \n",
    "    .. [Kovacs2005] Kovács, F., Legány, C., & Babos, A. (2005). Cluster validity measurement techniques. 6th International Symposium of Hungarian Researchers on Computational Intelligence.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = LabelEncoder().fit(labels).transform(labels)\n",
    "    \n",
    "    \n",
    "\n",
    "    ic_distances = inter_cluster_distances(labels, distances, cdist_method)\n",
    "    #print(\"IC\",ic_distances)\n",
    "    if len(ic_distances[ic_distances.nonzero()])==0:\n",
    "        min_distance = 0\n",
    "    else:\n",
    "        min_distance = min(ic_distances[ic_distances.nonzero()])\n",
    "    max_diameter = max(diameter(labels, distances, diameter_method))\n",
    "    \n",
    "    \n",
    "\n",
    "    return min_distance / max_diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "      <th>ACF1</th>\n",
       "      <th>Curvature</th>\n",
       "      <th>Peak</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Fspots</th>\n",
       "      <th>Cpoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.904494</td>\n",
       "      <td>5.239362e+01</td>\n",
       "      <td>8.135193e-01</td>\n",
       "      <td>-0.863262</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.978370e-01</td>\n",
       "      <td>172.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.781615</td>\n",
       "      <td>4.841265e+03</td>\n",
       "      <td>9.371226e-01</td>\n",
       "      <td>0.391511</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.698908e-01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.819130</td>\n",
       "      <td>1.042302e+04</td>\n",
       "      <td>9.285754e-01</td>\n",
       "      <td>-0.361289</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.317796e-01</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.306891</td>\n",
       "      <td>8.273701e+02</td>\n",
       "      <td>7.051710e-01</td>\n",
       "      <td>-0.677443</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.381565e-01</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.676847</td>\n",
       "      <td>1.191727e+04</td>\n",
       "      <td>9.103049e-01</td>\n",
       "      <td>-0.154082</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.524750e-01</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>33.333333</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>27.129680</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>23.540490</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>-0.144232</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>191.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>56.497175</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>10.999890</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Mean           Var          ACF1  Curvature          Peak  \\\n",
       "0     6.904494  5.239362e+01  8.135193e-01  -0.863262  1.000000e+00   \n",
       "1    44.781615  4.841265e+03  9.371226e-01   0.391511  1.000000e-07   \n",
       "2    44.819130  1.042302e+04  9.285754e-01  -0.361289  1.000000e-07   \n",
       "3    18.306891  8.273701e+02  7.051710e-01  -0.677443  1.000000e+00   \n",
       "4    59.676847  1.191727e+04  9.103049e-01  -0.154082  1.000000e-07   \n",
       "..         ...           ...           ...        ...           ...   \n",
       "195  33.333333  1.000000e-07  1.000000e-08  -0.016121  1.000000e-07   \n",
       "196  27.129680  1.000000e-07  1.000000e-08  -0.016121  1.000000e-07   \n",
       "197  23.540490  1.000000e-07  1.000000e-08  -0.144232  1.000000e-07   \n",
       "198  56.497175  1.000000e-07  1.000000e-08  -0.016121  1.000000e-07   \n",
       "199  10.999890  1.000000e-07  1.000000e-08  -0.016121  1.000000e-07   \n",
       "\n",
       "          Entropy  Fspots  Cpoints  \n",
       "0    5.978370e-01   172.0      6.0  \n",
       "1    1.698908e-01    49.0     10.0  \n",
       "2    1.317796e-01    58.0      2.0  \n",
       "3    5.381565e-01   120.0      3.0  \n",
       "4    1.524750e-01    84.0      4.0  \n",
       "..            ...     ...      ...  \n",
       "195  1.000000e-08   193.0      0.0  \n",
       "196  1.000000e-08   193.0      0.0  \n",
       "197  1.000000e-08   191.0      1.0  \n",
       "198  1.000000e-08   193.0      0.0  \n",
       "199  1.000000e-08   193.0      0.0  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Lshift', axis=1)\n",
    "df = df.drop('Trend', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "k=6\n",
    "n = df.shape[0]\n",
    "\n",
    "#Euclidean\n",
    "f_euclidean_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    #print(\"i\",i)\n",
    "    for j in range(1,n):\n",
    "         f_euclidean_dist[i,j] = euclidean(df.iloc[i].values.flatten(), df.iloc[j].values.flatten())\n",
    "\n",
    "#Corr\n",
    "corr_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "            corr_dist[i,j] = corr(df.iloc[i].values.flatten(), df.iloc[j].values.flatten())\n",
    "\n",
    "#scorr\n",
    "f_scorr_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        f_scorr_dist[i,j] = scorr(df.iloc[i].values.flatten(), df.iloc[j].values.flatten())\n",
    "#DTW\n",
    "f_dtw_dist = np.zeros((n,n))\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        f_dtw_dist[i,j] = fast_DTW(df.iloc[i].values.flatten(), df.iloc[j].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.        ,  4790.60230637, 10371.32689743, ...,\n",
       "           58.39849165,    75.39404559,    56.93505424],\n",
       "       [    0.        ,     0.        ,  5581.77196664, ...,\n",
       "         4843.40201391,  4843.43060407,  4843.53424341],\n",
       "       [    0.        ,  5581.77196664,     0.        , ...,\n",
       "        10423.89414255, 10423.90482321, 10423.953143  ],\n",
       "       ...,\n",
       "       [    0.        ,  4843.40201391, 10423.89414255, ...,\n",
       "            0.        ,    33.03270397,    12.73903654],\n",
       "       [    0.        ,  4843.43060407, 10423.90482321, ...,\n",
       "           33.03270397,     0.        ,    45.49728514],\n",
       "       [    0.        ,  4843.53424341, 10423.953143  , ...,\n",
       "           12.73903654,    45.49728514,     0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_euclidean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAC + euclidian distance: \n",
      "SC:  0.8369766894528083\n",
      "CHZ:  2742.276479517824\n",
      "DUNN:  1.2758936031303887\n",
      "DAV-BOUD:  0.3951131445254929\n",
      "HAC + corr distance: \n",
      "SC:  0.6344583094314\n",
      "CHZ:  1884.549681367708\n",
      "DUNN:  4.398782736275968\n",
      "DAV-BOUD:  0.4528659808654485\n",
      "HAC + scorr distance: \n",
      "SC:  0.42953476858245765\n",
      "CHZ:  154.50651615741378\n",
      "DUNN:  0.0\n",
      "DAV-BOUD:  0.9059638287740799\n",
      "HAC + dtw distance: \n",
      "SC:  0.8270391712953102\n",
      "CHZ:  2699.4201377837126\n",
      "DUNN:  0.3806698243475067\n",
      "DAV-BOUD:  0.34877169652112855\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\cluster\\hierarchy.py:826: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  return linkage(y, method='ward', metric='euclidean')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\cluster\\hierarchy.py:826: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  return linkage(y, method='ward', metric='euclidean')\n"
     ]
    }
   ],
   "source": [
    "#Experimentos HAC\n",
    "HAC_EUCLIDEAN=[]\n",
    "HAC_CORRELATION=[]\n",
    "HAC_SPEARMAN=[]\n",
    "HAC_DTW=[]\n",
    "\n",
    "HAC_EUCLIDEAN_CHZ=[]\n",
    "HAC_CORRELATION_CHZ=[]\n",
    "HAC_SPEARMAN_CHZ=[]\n",
    "HAC_DTW_CHZ=[]\n",
    "\n",
    "HAC_EUCLIDEAN_DUNN=[]\n",
    "HAC_CORRELATION_DUNN=[]\n",
    "HAC_SPEARMAN_DUNN=[]\n",
    "HAC_DTW_DUNN=[]\n",
    "\n",
    "HAC_EUCLIDEAN_DAVID=[]\n",
    "HAC_CORRELATION_DAVID=[]\n",
    "HAC_SPEARMAN_DAVID=[]\n",
    "HAC_DTW_DAVID=[]\n",
    "\n",
    "HAC_euc = AgglomerativeClustering(n_clusters=k).fit_predict(f_euclidean_dist)\n",
    "print(\"HAC + euclidian distance: \")\n",
    "sil = silhouette_score(f_euclidean_dist, HAC_euc)\n",
    "chz = calinski_harabasz_score(f_euclidean_dist, HAC_euc)\n",
    "dunn_= dunn(HAC_euc, f_euclidean_dist, 'farthest', 'farthest')\n",
    "david_= davies_bouldin_score(f_euclidean_dist, HAC_euc)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOUD: \", david_)\n",
    "HAC_EUCLIDEAN.append(sil)\n",
    "HAC_EUCLIDEAN_CHZ.append(chz)\n",
    "HAC_EUCLIDEAN_DUNN.append(dunn_)\n",
    "HAC_EUCLIDEAN_DAVID.append(david_)\n",
    "\n",
    "HAC_corr = AgglomerativeClustering(n_clusters=k).fit_predict(corr_dist)\n",
    "print(\"HAC + corr distance: \")\n",
    "sil = silhouette_score(corr_dist, HAC_corr)\n",
    "chz = calinski_harabasz_score(corr_dist, HAC_corr)\n",
    "dunn_ = dunn(HAC_corr, corr_dist, 'farthest', 'farthest')\n",
    "david_= davies_bouldin_score(corr_dist, HAC_corr)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOUD: \", david_)\n",
    "HAC_CORRELATION.append(sil)\n",
    "HAC_CORRELATION_CHZ.append(chz)\n",
    "HAC_CORRELATION_DUNN.append(dunn_)\n",
    "HAC_CORRELATION_DAVID.append(david_)\n",
    "\n",
    "HAC_scorr = AgglomerativeClustering(n_clusters=k).fit_predict(f_scorr_dist)\n",
    "print(\"HAC + scorr distance: \")\n",
    "sil = silhouette_score(f_scorr_dist, HAC_scorr)\n",
    "chz = calinski_harabasz_score(f_scorr_dist, HAC_scorr)\n",
    "dunn_ = dunn(HAC_scorr, f_scorr_dist, 'farthest', 'farthest')\n",
    "david_= davies_bouldin_score(f_scorr_dist, HAC_scorr)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOUD: \", david_)\n",
    "HAC_SPEARMAN.append(sil)\n",
    "HAC_SPEARMAN_CHZ.append(chz)\n",
    "HAC_SPEARMAN_DUNN.append(dunn_)\n",
    "HAC_SPEARMAN_DAVID.append(david_)\n",
    "\n",
    "HAC_dtw = AgglomerativeClustering(n_clusters=k).fit_predict(f_dtw_dist)\n",
    "print(\"HAC + dtw distance: \")\n",
    "sil = silhouette_score(f_dtw_dist, HAC_dtw)\n",
    "chz = calinski_harabasz_score(f_dtw_dist, HAC_dtw)\n",
    "dunn_ = dunn(HAC_dtw, f_dtw_dist, 'farthest', 'farthest')\n",
    "david_= davies_bouldin_score(f_dtw_dist, HAC_dtw)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOUD: \", david_)\n",
    "HAC_DTW.append(sil)\n",
    "HAC_DTW_CHZ.append(chz)\n",
    "HAC_DTW_DUNN.append(dunn_)\n",
    "HAC_DTW_DAVID.append(david_)\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KM + euclidian distance: \n",
      "SC:  0.8630193861766073\n",
      "CHZ:  3238.9815842670737\n",
      "DUNN:  1.47465467510902\n",
      "DAV-BOULD:  0.3311495936791166\n",
      "KM + corr distance: \n",
      "SC:  0.65776177537447\n",
      "CHZ:  2250.273033710479\n",
      "DUNN:  4.398782736275968\n",
      "DAV-BOULD:  0.4640018908448443\n",
      "KM + scorr distance: \n",
      "SC:  0.4524138284457876\n",
      "CHZ:  160.6980835802667\n",
      "DUNN:  0.0\n",
      "DAV-BOULD:  0.8905704947659673\n",
      "KM + dtw distance: \n",
      "SC:  0.8610639920618633\n",
      "CHZ:  3164.9675082103136\n",
      "DUNN:  1.4928846596054839\n",
      "DAV-BOULD:  0.32984782742432733\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "KM_EUCLIDEAN=[]\n",
    "KM_CORRELATION=[]\n",
    "KM_SPEARMAN=[]\n",
    "KM_DTW=[]\n",
    "\n",
    "KM_EUCLIDEAN_CHZ=[]\n",
    "KM_CORRELATION_CHZ=[]\n",
    "KM_SPEARMAN_CHZ=[]\n",
    "KM_DTW_CHZ=[]\n",
    "\n",
    "KM_EUCLIDEAN_DUNN=[]\n",
    "KM_CORRELATION_DUNN=[]\n",
    "KM_SPEARMAN_DUNN=[]\n",
    "KM_DTW_DUNN=[]\n",
    "\n",
    "KM_EUCLIDEAN_DAVID=[]\n",
    "KM_CORRELATION_DAVID=[]\n",
    "KM_SPEARMAN_DAVID=[]\n",
    "KM_DTW_DAVID=[]\n",
    "#Experimentos K-Means\n",
    "km_euc = KMeans(n_clusters=k).fit_predict(f_euclidean_dist)\n",
    "print(\"KM + euclidian distance: \")\n",
    "sil = silhouette_score(f_euclidean_dist, km_euc)\n",
    "chz = calinski_harabasz_score(f_euclidean_dist, km_euc)\n",
    "dunn_ = dunn(km_euc, f_euclidean_dist, 'farthest', 'farthest')\n",
    "david_ = davies_bouldin_score(f_euclidean_dist, km_euc)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "KM_EUCLIDEAN.append(sil)\n",
    "KM_EUCLIDEAN_CHZ.append(chz)\n",
    "KM_EUCLIDEAN_DUNN.append(dunn_)\n",
    "KM_EUCLIDEAN_DAVID.append(david_)\n",
    "\n",
    "km_corr = KMeans(n_clusters=k).fit_predict(corr_dist)\n",
    "print(\"KM + corr distance: \")\n",
    "sil = silhouette_score(corr_dist, km_corr)\n",
    "chz = calinski_harabasz_score(corr_dist, km_corr)\n",
    "dunn_ = dunn(km_corr, corr_dist, 'farthest', 'farthest') \n",
    "david_ = davies_bouldin_score(corr_dist, km_corr)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "KM_CORRELATION.append(sil)\n",
    "KM_CORRELATION_CHZ.append(chz)\n",
    "KM_CORRELATION_DUNN.append(dunn_)\n",
    "KM_CORRELATION_DAVID.append(david_)\n",
    "\n",
    "km_scorr = KMeans(n_clusters=k).fit_predict(f_scorr_dist)\n",
    "print(\"KM + scorr distance: \")\n",
    "sil = silhouette_score(f_scorr_dist, km_scorr)\n",
    "chz = calinski_harabasz_score(f_scorr_dist, km_scorr)\n",
    "dunn_ = dunn(km_scorr, f_scorr_dist, 'farthest', 'farthest')\n",
    "david_ = davies_bouldin_score(f_scorr_dist, km_scorr)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "KM_SPEARMAN.append(sil)\n",
    "KM_SPEARMAN_CHZ.append(chz)\n",
    "KM_SPEARMAN_DUNN.append(dunn_)\n",
    "KM_SPEARMAN_DAVID.append(david_)\n",
    "\n",
    "km_dtw = KMeans(n_clusters=k).fit_predict(f_dtw_dist)\n",
    "print(\"KM + dtw distance: \")\n",
    "sil = silhouette_score(f_dtw_dist, km_dtw)\n",
    "chz = calinski_harabasz_score(f_dtw_dist, km_dtw)\n",
    "dunn_ = dunn(km_dtw, f_dtw_dist, 'farthest', 'farthest')\n",
    "david_ = davies_bouldin_score(f_dtw_dist, km_dtw)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", chz)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "KM_DTW.append(sil)\n",
    "KM_DTW_CHZ.append(chz)\n",
    "KM_DTW_DUNN.append(dunn_)\n",
    "KM_DTW_DAVID.append(david_)\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN + euclidian distance: \n",
      "SC:  0.4463369064835646\n",
      "CHZ:  9.566139077117597\n",
      "DUNN:  0.001842577705548914\n",
      "DAV-BOULD:  1.06687713947852\n",
      "DBSCAN + corr distance: \n",
      "SC:  0.3776484047793511\n",
      "CHZ:  485.21250628087097\n",
      "DUNN:  3.314028817355511\n",
      "DAV-BOULD:  0.484932541025244\n",
      "DBSCAN + scorr distance: \n",
      "SC:  0.7156020263272871\n",
      "CHZ:  45.61881779252594\n",
      "DUNN:  0.0\n",
      "DAV-BOULD:  1.0072510675340651\n",
      "DBSCAN + dtw distance: \n",
      "SC:  0.3878543960343437\n",
      "CHZ:  12.599769104464395\n",
      "DUNN:  0.0037372820836774887\n",
      "DAV-BOULD:  1.1985423045624772\n"
     ]
    }
   ],
   "source": [
    "DBSCAN_EUCLIDEAN=[]\n",
    "DBSCAN_CORRELATION=[]\n",
    "DBSCAN_SPEARMAN=[]\n",
    "DBSCAN_DTW=[]\n",
    "\n",
    "DBSCAN_EUCLIDEAN_CHZ=[]\n",
    "DBSCAN_CORRELATION_CHZ=[]\n",
    "DBSCAN_SPEARMAN_CHZ=[]\n",
    "DBSCAN_DTW_CHZ=[]\n",
    "\n",
    "DBSCAN_EUCLIDEAN_DUNN=[]\n",
    "DBSCAN_CORRELATION_DUNN=[]\n",
    "DBSCAN_SPEARMAN_DUNN=[]\n",
    "DBSCAN_DTW_DUNN=[]\n",
    "\n",
    "DBSCAN_EUCLIDEAN_DAVID=[]\n",
    "DBSCAN_CORRELATION_DAVID=[]\n",
    "DBSCAN_SPEARMAN_DAVID=[]\n",
    "DBSCAN_DTW_DAVID=[]\n",
    "\n",
    "#CON EUCLIDEAN\n",
    "DB_euc = DBSCAN(eps=500, min_samples=3).fit_predict(f_euclidean_dist)\n",
    "print(\"DBSCAN + euclidian distance: \")\n",
    "sil =  silhouette_score(f_euclidean_dist, DB_euc)\n",
    "CHZ_ = calinski_harabasz_score(f_euclidean_dist, DB_euc) \n",
    "dunn_ = dunn(DB_euc, f_euclidean_dist, 'farthest', 'farthest')\n",
    "david_ = davies_bouldin_score(f_euclidean_dist, DB_euc)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", CHZ_)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "DBSCAN_EUCLIDEAN.append(sil)\n",
    "DBSCAN_EUCLIDEAN_CHZ.append(CHZ_)\n",
    "DBSCAN_EUCLIDEAN_DUNN.append(dunn_)\n",
    "DBSCAN_EUCLIDEAN_DAVID.append(david_)\n",
    "\n",
    "#CON CORRELATION\n",
    "DB_corr = DBSCAN(eps=0.45, min_samples=3).fit_predict(corr_dist)\n",
    "print(\"DBSCAN + corr distance: \")\n",
    "sil = silhouette_score(corr_dist, DB_corr)\n",
    "print(\"SC: \", sil)\n",
    "CHZ_ = calinski_harabasz_score(corr_dist, DB_corr)\n",
    "print(\"CHZ: \", CHZ_)\n",
    "dunn_ = dunn(DB_corr, corr_dist, 'farthest', 'farthest')\n",
    "print(\"DUNN: \", dunn_)\n",
    "david_ = davies_bouldin_score(corr_dist, DB_corr)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "DBSCAN_CORRELATION.append(sil)\n",
    "DBSCAN_CORRELATION_CHZ.append(CHZ_)\n",
    "DBSCAN_CORRELATION_DUNN.append(dunn_)\n",
    "DBSCAN_CORRELATION_DAVID.append(david_)\n",
    "\n",
    "#CON SPEARMAN\n",
    "DB_scorr = DBSCAN(eps=0.45, min_samples=3).fit_predict(f_scorr_dist)\n",
    "print(\"DBSCAN + scorr distance: \")\n",
    "sil = silhouette_score(f_scorr_dist, DB_scorr)\n",
    "print(\"SC: \", sil)\n",
    "CHZ_ = calinski_harabasz_score(f_scorr_dist, DB_scorr)\n",
    "print(\"CHZ: \", CHZ_)\n",
    "dunn_ = dunn(DB_scorr, f_scorr_dist, 'farthest', 'farthest')\n",
    "print(\"DUNN: \", dunn_)\n",
    "david_ = davies_bouldin_score(f_scorr_dist, DB_scorr)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "DBSCAN_SPEARMAN.append(sil)\n",
    "DBSCAN_SPEARMAN_CHZ.append(CHZ_)\n",
    "DBSCAN_SPEARMAN_DUNN.append(dunn_)\n",
    "DBSCAN_SPEARMAN_DAVID.append(david_)\n",
    "\n",
    "#CON D TIME WARPING\n",
    "DB_dtw = DBSCAN(eps=500, min_samples=3).fit_predict(f_dtw_dist)\n",
    "print(\"DBSCAN + dtw distance: \")\n",
    "sil = silhouette_score(f_dtw_dist, DB_dtw)\n",
    "CHZ_ = calinski_harabasz_score(f_dtw_dist, DB_dtw)\n",
    "dunn_ = dunn(DB_dtw, f_dtw_dist, 'farthest', 'farthest')\n",
    "david_ = davies_bouldin_score(f_dtw_dist, DB_dtw)\n",
    "print(\"SC: \", sil)\n",
    "print(\"CHZ: \", CHZ_)\n",
    "print(\"DUNN: \", dunn_)\n",
    "print(\"DAV-BOULD: \", david_)\n",
    "DBSCAN_DTW.append(sil)\n",
    "DBSCAN_DTW_CHZ.append(CHZ_)\n",
    "DBSCAN_DTW_DUNN.append(dunn_)\n",
    "DBSCAN_DTW_DAVID.append(david_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores = pd.DataFrame()\n",
    "Scores['Metrica'] = ['Euclidean Distance', 'Pearson Correlation', 'Spearman Correlation', 'Dynamic Time Warping'] \n",
    "Scores['SIL HAC 8'] = [ HAC_EUCLIDEAN[0], HAC_CORRELATION[0], HAC_SPEARMAN[0], HAC_DTW[0]]\n",
    "Scores['SIL KM 8'] = [ KM_EUCLIDEAN[0], KM_CORRELATION[0], KM_SPEARMAN[0], KM_DTW[0]]\n",
    "Scores['SIL DB 8'] = [ DBSCAN_EUCLIDEAN[0], DBSCAN_CORRELATION[0], DBSCAN_SPEARMAN[0], DBSCAN_DTW[0]]\n",
    "\n",
    "Scores['CHZ HAC 8'] = [ HAC_EUCLIDEAN_CHZ[0], HAC_CORRELATION_CHZ[0], HAC_SPEARMAN_CHZ[0], HAC_DTW_CHZ[0]]\n",
    "Scores['CHZ KM 8'] = [ KM_EUCLIDEAN_CHZ[0], KM_CORRELATION_CHZ[0], KM_SPEARMAN_CHZ[0], KM_DTW_CHZ[0]]\n",
    "Scores['CHZ DB 8'] = [ DBSCAN_EUCLIDEAN_CHZ[0], DBSCAN_CORRELATION_CHZ[0], DBSCAN_SPEARMAN_CHZ[0], DBSCAN_DTW_CHZ[0]]\n",
    "\n",
    "Scores['DAVIES HAC 8'] = [ HAC_EUCLIDEAN_DAVID[0], HAC_CORRELATION_DAVID[0], HAC_SPEARMAN_DAVID[0], HAC_DTW_DAVID[0]]\n",
    "Scores['DAVIES KM 8'] = [ KM_EUCLIDEAN_DAVID[0], KM_CORRELATION_DAVID[0], KM_SPEARMAN_DAVID[0], KM_DTW_DAVID[0]]\n",
    "Scores['DAVIES DB 8'] = [ DBSCAN_EUCLIDEAN_DAVID[0], DBSCAN_CORRELATION_DAVID[0], DBSCAN_SPEARMAN_DAVID[0], DBSCAN_DTW_DAVID[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores.to_csv('Scores_FeatureSelection_F8K6.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux2 = pd.DataFrame()\n",
    "aux2['Distrito'] = listadistrito\n",
    "aux2['Cluster KM'] = km_euc\n",
    "aux2['Cluster HAC '] = HAC_euc\n",
    "aux2['Cluster DB SP'] = DB_scorr\n",
    "aux2.to_csv('ClusterFBFSK6_2009al2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
